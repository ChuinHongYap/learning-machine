<!DOCTYPE HTML>
<html lang="zh" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>學習機器</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="要做機器學習，必先學習機器">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="intro.html">Introduction</a></li><li class="chapter-item expanded "><a href="getting-started/overview.html"><strong aria-hidden="true">1.</strong> Getting Started</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="getting-started/funcs.html"><strong aria-hidden="true">1.1.</strong> Models</a></li><li class="chapter-item expanded "><a href="getting-started/obj.html"><strong aria-hidden="true">1.2.</strong> Objectives</a></li><li class="chapter-item expanded "><a href="getting-started/optim.html"><strong aria-hidden="true">1.3.</strong> Optimization</a></li><li class="chapter-item expanded "><a href="getting-started/linear-case.html"><strong aria-hidden="true">1.4.</strong> Case Study: Linear Models</a></li><li class="chapter-item expanded "><a href="getting-started/ccc.html"><strong aria-hidden="true">1.5.</strong> CCC</a></li></ol></li><li class="chapter-item expanded "><a href="what-we-want/overview.html"><strong aria-hidden="true">2.</strong> What we want</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="what-we-want/correctness.html"><strong aria-hidden="true">2.1.</strong> Correctness</a></li><li class="chapter-item expanded "><a href="what-we-want/simplicity.html"><strong aria-hidden="true">2.2.</strong> Simplicity</a></li><li class="chapter-item expanded "><a href="what-we-want/ccc.html"><strong aria-hidden="true">2.3.</strong> CCC</a></li></ol></li><li class="chapter-item expanded "><a href="digging-deeper/overview.html"><strong aria-hidden="true">3.</strong> Digging Deeper</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="digging-deeper/no-myths.html"><strong aria-hidden="true">3.1.</strong> No More Myths</a></li><li class="chapter-item expanded "><a href="digging-deeper/approx.html"><strong aria-hidden="true">3.2.</strong> Approximation</a></li><li class="chapter-item expanded "><a href="digging-deeper/ccc.html"><strong aria-hidden="true">3.3.</strong> CCC</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">學習機器</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        <a href="https://github.com/r3ntru3w4n9/learning-machine/tree/master/zh" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="intro-to-machine-learning"><a class="header" href="#intro-to-machine-learning">Intro to machine learning</a></h1>
<p><em><strong>Whoever fights monsters should see to it that in the process he does not become a monster. And if you gaze long enough into an abyss, the abyss will gaze back into you. And in order to tame machine learning, one mush first know how to learn machine.</strong></em>
--- Me, 2021</p>
<h2 id="what-exactly-is-machine-learning"><a class="header" href="#what-exactly-is-machine-learning">What exactly is machine learning?</a></h2>
<p><em>&quot;Machine learning (ML) is the study of computer algorithms that improve automatically through experience and by the use of data. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as &quot;training data&quot;, in order to make predictions or decisions without being explicitly programmed to do so.&quot;</em> --- Wikipedia</p>
<p>Woah, woah! That's a mouthful! What is it supposed to mean?</p>
<p>Machines do what programmers tell it to do. Exactly. Without exception. If the machine didn't do what you want it to do, then you are usually the one to blame. Normally we call this a <em>bug</em>.</p>
<p>But Wikipedia says <em>&quot;make predictions or decisions without being explicitly programmed to do so.&quot;</em> How can I write a program without telling it what to do? Does it automatically do what I want it to do?Well, sort of. Let me elaborate on that. </p>
<h2 id="why-is-machine-learning-helpful"><a class="header" href="#why-is-machine-learning-helpful">Why is machine learning helpful?</a></h2>
<p>Usually when a program is created, the programmer writing it would design the program to do specific things. Your &quot;Hello World&quot; program is designed to log strings to the console. We all have the experience of finding a computer program, a website, or a mobile app, so dumb it's infuriating to use. It's likely because the designers of the program have failed to cover all the cases that a user might want to use it. <strong>In other words, they haven't seen your use of the app, so they didn't consider it used this way.</strong> This is where machine learnings might come to the rescue!</p>
<pre><code class="language-python">def hello_world():
    &quot;This is guaranteed to print 'Hello World!'&quot;
    print(&quot;Hello World!&quot;)
</code></pre>
<p>Alternatively, we design a program that's extensible, it acts according to data it receives. It <em>learns</em> what is given to it. Now our problem of programs not doing what the users want is no more! Our program now automatically adapts to your users. And we have eliminated a lot of edge cases that we previously would have to write.</p>
<p>An example of that would be the modern translation system. Sure, anyone knows how 'good' can be translated into 'bien' and 'morning' can be translated into 'Matin', but 'good morning' is translated into 'bonjour' instead of 'bien Matin', which doesn't make any sense! And there is just not a simple rule for it. <strong>But the power of machine learning is to generalize, and to extend it beyond what is known.</strong> With it, which we'll cover &quot;how&quot; down the book, you will see how this is done without explicitly programmed into the program.</p>
<p>But the benefits doesn't just stop here. The generalization of machine learning is so powerful it can aid in problems where programmers often don't even know where to start. Say, telling bananas from apples. It's a simple enough task but very difficult to be described in simple constructs (if, else, for...), so much that before machine learning this is nearly impossible to do. Again, you'll see how to solve this problem with machine learning in the book.</p>
<pre><code class="language-python">
</code></pre>
<p>As there are a lot of content to cover in the whole machine learning field, in this book we will mainly cover what's especially popular in the last couple of years, deep learning. In this book, we'll use machine learning and deep learning sort of interchangabley.</p>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>If we only care about seen data, we could simply use a mapping and call it a day. <strong>The reason machine learning is so powerful is because how well it can generalize on previously unseen data.</strong></p>
<h1 id="getting-started-into-machine-learning"><a class="header" href="#getting-started-into-machine-learning">Getting started into machine learning</a></h1>
<p>We have talked about what machine learning can do in the introduction. However, we've not yet discussed how we are going to do that. In this chapter we are going to briefly talk about the general flow and essential parts of all machine learning problems, you can think this as a formula for problems you solve.</p>
<p>The <em>Holy Trinity</em> of machine learning:</p>
<h2 id="1-a-hrefgetting-startedfuncshtmlmodela"><a class="header" href="#1-a-hrefgetting-startedfuncshtmlmodela">1. <a href="getting-started/./funcs.html">Model</a></a></h2>
<p>Every heard of machine learning models? Models <em>are</em> how super smart data scientists call functions. We are humble though.</p>
<h2 id="2-a-hrefgetting-startedobjhtmlobjectivea"><a class="header" href="#2-a-hrefgetting-startedobjhtmlobjectivea">2. <a href="getting-started/./obj.html">Objective</a></a></h2>
<p>How do we determine what is a good function? After all, there are so many ways to determine what is a good function.</p>
<h2 id="3-a-hrefgetting-startedoptimhtmloptimizationa"><a class="header" href="#3-a-hrefgetting-startedoptimhtmloptimizationa">3. <a href="getting-started/./optim.html">Optimization</a></a></h2>
<p>For a good function to be useful, we have to understand how to get them. Miracles don't happen by themselves.</p>
<h2 id="a-hrefgetting-startedlinear-casehtmlcase-studya"><a class="header" href="#a-hrefgetting-startedlinear-casehtmlcase-studya"><a href="getting-started/./linear-case.html">Case Study</a></a></h2>
<p>Here's an example if you prefer to see an example first. Please remember to come back for formal definitions.</p>
<h1 id="using-models-to-make-predictions"><a class="header" href="#using-models-to-make-predictions">Using models to make predictions</a></h1>
<p><em>In machine learning we want to build a model (a function) to do what we want.</em></p>
<p>You must have heard of machine learning models doing amazing things. But what exactly is a model? A model takes in some input and spit out some output. They are like functions we write in programs. You might have heard that some machine learning models know how to read a picture and translate them into words. In this case the input is the picture, and the output is the content of a picture in words we understand. Or maybe you have heard of how models can predict the stock market (while this is true, it's not that useful since the prediction may not be accurate, or else data scientists get so rich they no longer do data science), in such case the input previous days or years of share prices, the output tomorrow's share price.</p>
<pre><code class="language-python">    I mean, if you don't know how machine learning works,
    You cannot tell the difference between this and a real working method!
    &quot;&quot;&quot;
    return x.sum()
</code></pre>
<p>The possibilities of different uses of models is endless. But there are only <em>2 kinds</em> of models, <strong>classification</strong>, and <strong>regression</strong>. What are those things?</p>
<h2 id="classification"><a class="header" href="#classification">Classification</a></h2>
<p><em>Given some input, we want to separate those input data into different classes.</em></p>
<p>A concrete example of the above sentence: Suppose that we have several pictures of dogs and cats, and we want to separate them into two piles. One pile for dogs, and the other for cats. In machine learning, we could simply train a <strong>classification model</strong>, and the image is transformed by the model into two categories, dogs and cats. This task is called <strong>classification</strong> by us super smart machine learning masters.</p>
<pre><code class="language-python">    Returns a numbers to tell your confidence in the image being a cat instead of a dog.
    In this function how likely it is a cat is affected by the image brightness.
    Obviously not a good way.
    &quot;&quot;&quot;
    result = image.sum() / image.size
    assert 0.0 &lt;= result &lt;= 1.0
    return result
</code></pre>
<p>In the above example, since there are only two categories, we can easily use a number to represent our confidence in the input being one of the classes, cats. The problem is referred to as a <strong>binary classification problem</strong>. However, what happens when there are more than two categories?</p>
<pre><code class="language-python">    &quot;&quot;&quot;
    There are three categories. Cats. Dogs. Rabbits.
    So normally we will output an array of length 3 showing how possible the input is of each class.
    Do note that the array sums to 1 because the sum of probablities of all possible cases is 1.
    &quot;&quot;&quot;
    return np.array([0.6, 0.2, 0.2])
</code></pre>
<p>We use an array to represent probabilities of different classes. It's usually called a <strong>categorical classification problem</strong>. To identify if a problem is a classification problem, think this: <em>am I predicting classes?</em></p>
<h2 id="regression"><a class="header" href="#regression">Regression</a></h2>
<p><em>Given some input, we want to predict the score of the input data.</em> Usually, the score is unbounded.</p>
<p>A concrete example of the above sentence: Suppose that we have an image of a star. We want to predict how hot is the surface (since we data scientists don't study physics and machine learning is cooler anyways), how are we going to do it?</p>
<pre><code class="language-python">    since the return value does not have a range.
    &quot;&quot;&quot;
    return 10000 * image.sum() / image.size
</code></pre>
<p>The output may not even be scalar. It can be an array as well! Predicting scalar is much more common though. To tell part classification from regression, just keep this in mind: <em>regression predicts value.</em></p>
<h2 id="summary-1"><a class="header" href="#summary-1">Summary</a></h2>
<p>All problems in machine learning fall into 2 categories, <em>classification</em> and <em>regression</em>. The difference is that <em>classification</em> are used to predict different categories, while <em>regression</em> is used to predict a value. Since classification is usually solved by predicting probabilities, a direct result is that the output of a classification model is often bounded, while a regression model is not.</p>
<h1 id="what-is-a-better-function"><a class="header" href="#what-is-a-better-function">What is a better function?</a></h1>
<p><em>We need a way to tell us that a function is good.</em></p>
<p>Let's be honest, we are learning machine learning for some serious, important, big brain things. We want the model to be as good as possible at what it does. But what is good? </p>
<pre><code class="language-python">def f(x):
    &quot;The function looks good&quot;
    return 3 * x + 5


def g(x):
    &quot;The function also looks good&quot;
    return 8 * x - 1
</code></pre>
<p>A clever way to say one model is better than another model is to compare those models. <em>Turns out, in math, it's illegal to compare different vectors (an array of numbers), so our options are limited to scalars (single numbers).</em> But what scalar? </p>
<h2 id="loss"><a class="header" href="#loss">Loss</a></h2>
<p>In machine learning, there are a lot of tasks tackling real world problems. Such as understanding images. Such as predicting the next champion team of the world cup. There are too many possibilities in the real world. To deal with that problem, scientists basically give up. They stop trying to create a new prefect model. Instead, they focus on a model that solves problems good enough. They use a scalar to determine how far away the model's prediction of a known problem is different from the right answer, it's very common to call the scalar <strong>loss</strong>.</p>
<p>The smaller the loss, the better the function is. Usually the loss is a positive number, but occasionally we see negative losses permitted in a problem. As long a smaller loss indicates a better function. To calculate loss, we define a <strong>loss function</strong>. <em>The loss function is defined to operate on the function itself (it's a function of function) and output how good the model is. Realistically though, the loss function operates on the output of the function and determines how good that output is.</em> To further clarify what I mean</p>
<h3 id="loosely-defined-loss-function-the-one-used-in-pytorch-tensorflow-etc"><a class="header" href="#loosely-defined-loss-function-the-one-used-in-pytorch-tensorflow-etc">Loosely defined loss function (the one used in PyTorch, TensorFlow, etc):</a></h3>
<pre><code class="language-python">    return ((x - y) ** 2).sum()


def loss_func(F, real_x, real_y):
    &quot;&quot;&quot;
    loss_func calls the function for you,
</code></pre>
<h3 id="formally-defined-loss-function"><a class="header" href="#formally-defined-loss-function">Formally defined loss function:</a></h3>
<pre><code class="language-python">
    pred_y = F(real_x)
    loss = distance(pred_y, real_y)

    # Optimize the function based on loss
    optimize_function(F)

    return loss


def optimize_function(F):
    &quot;We are doing something to `F`'s parameter and hope that the function gets better&quot;
    F.parameter = 0
</code></pre>
<p>We will talk about how to optimize the function in the next chapter.</p>
<p>In practice you often see the loosely defined loss function instead of the formally defined loss function. After all it's more flexible, and is better suited to be provided as a library function. However, since models and losses are discussed all together in the optimization phase, we will use the formally defined loss function in this book.</p>
<h2 id="several-commonly-used-loss-functions"><a class="header" href="#several-commonly-used-loss-functions">Several commonly used loss functions:</a></h2>
<h3 id="mean-absolute-error"><a class="header" href="#mean-absolute-error">Mean absolute error:</a></h3>
<p>Used in classification as well as regression problems.</p>
<p>Formula:</p>
<p>\[L(x, y) = \sum_{i=1}^n |x_i - y_i|\]
\[x, y \in R^n \]</p>
<p>Code:</p>
<pre><code class="language-python">    &quot;MAE: mean absolute error. Mean of absolute difference.&quot;
    return np.abs(x - y).mean()
</code></pre>
<h3 id="mean-squared-error"><a class="header" href="#mean-squared-error">Mean squared error:</a></h3>
<p>Used in classification as well as regression problems.</p>
<p>Formula:</p>
<p>\[L(x, y) = \sum_{i=1}^n (x_i - y_i)^2\]
\[x, y \in R^n \]</p>
<p>Code:</p>
<pre><code class="language-python">    &quot;MSE: mean absolute error. Mean of squared difference.&quot;
    return np.power(x - y, 2).mean()
</code></pre>
<h4 id="cross-entropy-loss"><a class="header" href="#cross-entropy-loss">Cross-entropy loss:</a></h4>
<p>Used in classification problems only.</p>
<p>Formula:</p>
<p>\[L(x, y) = - \sum_{i=1}^n x_i \log_2 y_i\]
\[x_i, y_i \in [0, 1] \]</p>
<p>Code:</p>
<pre><code class="language-python">    &quot;CE: Cross entropy. You will learn about this later.&quot;
    return -(x * np.log2(y)).sum()
</code></pre>
<h2 id="summary-2"><a class="header" href="#summary-2">Summary</a></h2>
<p>To define a good function, we have to <strong>find the loss that for this function</strong>. . To find the loss of a function, we <strong>use a loss function to find out the loss</strong>. <strong>A good loss function shows far how the output of the model is from a good output</strong>.</p>
<h1 id="how-to-get-a-great-function"><a class="header" href="#how-to-get-a-great-function">How to get a great function?</a></h1>
<p><em>Knowing how good something is is no good, unless we can get a good one.</em></p>
<p>We have a model. We know how good it is. Now, how do we find a good model? A naive way of trying to achieve that is to randomly choose parameters and cross our fingers and hope that the model is good enough. However, the number of possible parameters can be crazily large, so it's not always feasible solution. What is a good way to solve the problem?</p>
<h2 id="gradient-descent"><a class="header" href="#gradient-descent">Gradient descent</a></h2>
<p>Imagine you are standing on a hill. You want to find the lowest point in your region, how would you do it? You take a ball, and roll it down the hill, and naturally the ball stops at the place where the height is minimum. This is exactly what mathematicians decided to use to solve the problem. And it has a cool name. <strong>Gradient descent</strong>.</p>
<p>Wait, you ask. How is this rolling downhill related to gradient descent? Well, let's define a function which we want to optimize \(f\), and we define</p>
<p>\[z = f(x, y)\]
\[z, x, y \in R\]</p>
<p>In other words, given there is a location on the 2D plane \(xy\), the function takes in the position and output a scalar. If we say that the scalar is the height at \((x, y)\), then if we plot \((x, y, z)\) in our head, it looks like a terrain. And if we &quot;roll a ball&quot; down from any location, the ball is eventually going to find a minimum (because of physics).</p>
<p>Now let's generalize the idea. Imagine our function \(f\) is no longer limited to 2 inputs. Let's substitute \(f\) for a loss function \(L\) we discussed last chapter (the formal one). </p>
<p>The input is a function \(f\) that we want to optimize (the ball that we want to roll down the hill), and a pair of inputs \((x, y)\), given that \(n \in N\), \(x, y \in R^n\)</p>
<p>Remember that we've said over and over that the function is determined by it's parameters? To optimize a function, we optimize its parameter. In this case, <strong>the function parameters</strong> is the position of the ball, and the <strong>loss function output</strong> is the value we want to minimize.</p>
<pre><code class="language-python">
    pred_y = F(real_x)
    loss = distance(pred_y, real_y)

    # Optimize the function based on loss
    optimize_function(F)

    return loss


def optimize_function(F):
    &quot;We are doing something to `F`'s parameter and hope that the function gets better&quot;
    F.parameter = 0
</code></pre>
<p>Now we get to the gradient part. Warning: A little math here.</p>
<p>First, let's talk about what gradients are. Note that the weird upside-down triangle is the notation for gradient.</p>
<p>Let's suppose there is a function.</p>
<p>\[y = f(x_1, x_2, ... , x_n)\]
\[y, x_1, x_2 , ... , y_n, n \in R\]</p>
<p>Then we say that the gradient of \(y\):</p>
<p>\[\nabla y = \frac{\partial y}{\partial x} = (\frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2}, ..., \frac{\partial y}{\partial x_n})^T\]</p>
<p>And since \(\Delta x\) is</p>
<p>\[\Delta x = (\partial x_1, \partial x_2, ..., \partial x_n)^T\]</p>
<p>If we now make the dot product of \(\nabla y\) and  a small change in \(x\), denoted by \(\Delta x\), we observe:</p>
<p>\[(\nabla y)^T(\Delta x) = \frac{\partial y}{\partial x_1}\partial x_1 + \frac{\partial y}{\partial x_2}\partial x_2 + ... + \frac{\partial y}{\partial x_n}\partial x_n = \sum_{i=1}^n \Delta y_i = \Delta y\]</p>
<p>The multiplication of the gradient of \(y\) and a small change in \(x\) yields a small change in \(y\)! What this mean is that if we change \(x\) by a little bit \(\Delta x\), we can observe the change in \(y\) being \(\Delta y\). This is why gradients are so useful.</p>
<p><em>You can think of gradient as the steepest way uphill.</em> If you move along the gradient, you can increase the function value the fastest, in this case, you will increase the <strong>loss</strong>. So usually we want to move in the opposite direction of the gradient at a point (remember we are minimizing the function?). If we always move to the opposite direction of the gradient, we will eventually get a (local) minimum value of the function, we call this <strong>stochastic gradient descent</strong>.</p>
<h3 id="stochastic-gradient-descent"><a class="header" href="#stochastic-gradient-descent">Stochastic gradient descent</a></h3>
<p>Stochastic gradient descent is the most common gradient descent method, and the most basic one. It involves only 3 variables, \(x, y, lr, iter\). \(x\) is your input variable, \(y\) is your output variable, and \(lr\), learning rate, is a scalar showing how far you can move in one iteration. You repeat \(iter\) iterations. For the ultimate quality we would want \(lr\) to be as small as possible (to not jump too far because you want to follow the terrain), and as many \(iter\) as possible (to ensure that the minimum is obtained). But it's not possible to do so in practice. Selecting \(lr\) and \(iter\) has become more art than science, and is often called <strong>hyperparameter tuning</strong> (parameter is for functions).</p>
<pre><code class="language-python">def sgd_once(x, y, lr):
    &quot;&quot;&quot;
    sgd stands for stochastic gradient descent.
    It moves in the direct opposite of the gradient.
    lr means learning rate.
    It is a small number because we don't want to move too far in a step.
    &quot;&quot;&quot;
    loss = loss_func(x, y)

    # This is the gradient descent step
    x -= lr * 2.0 * (x - y)

    new_loss = loss_func(x, y)

    # A successful step should mean that we get a smaller new loss
    assert new_loss &lt; loss


def sgd(x: ndarray, y: ndarray, lr: float, iter: int):
    &quot;&quot;&quot;
    A real sgd flow doesn't just make one step.
    It usually updates for several hundreds or even thousands of iterations
    &quot;&quot;&quot;
    for _ in range(iter):
        sgd_once(x, y, lr)
</code></pre>
<h2 id="summary-3"><a class="header" href="#summary-3">Summary</a></h2>
<p>We choose an initial <strong>function</strong> by selecting a parameter (randomly), then optimize it by gradient descent, the value we want to reduce is the <strong>loss</strong>, and the parameters we want to choose is the <strong>function's parameters</strong>. Gradient descent is a lot like rolling balls down the hills, in which the ball will find the lowest point, which is the minimum.</p>
<h1 id="case-study-linear-models"><a class="header" href="#case-study-linear-models">Case study: Linear models</a></h1>
<p>We all know linear models. (Right...? Guys?) A linear model has the form \(y = wx + b\), with \(x, y, w, b \in R\). In data science though, a more fancy way to write this equation is \(Y = WX + B\), with \(X \in R^m\) \(Y, B \in R^n\), \(W \in R^{mn}\). You will need a little bit of linear algebra to understand this. Sorry. The reason scientists like to write things in matrix and vector form is that this way they can write a lot of equations in one place. Now we ask ourselves, where are the <em>Holy Trinity</em> we just spent a lot of time on in previous chapters?</p>
<h2 id="1-a-hrefgetting-startedfuncshtmlmodelsa"><a class="header" href="#1-a-hrefgetting-startedfuncshtmlmodelsa">1. <a href="getting-started/./funcs.html">Models</a></a></h2>
<p>This one is simple. We define \(F(X) = WX + B\), with \(X \in R^m\), and \(Y, B \in R^n\), and \(W \in R^{mn}\). Then <strong>\(F\) is our model</strong>.</p>
<p>Code:</p>
<pre><code class="language-python">class Linear(object):
    &quot;A linear model has the form F(x) = W @ X + B&quot;
    Grad = namedtuple(&quot;Grad&quot;, [&quot;w&quot;, &quot;b&quot;])

    def __init__(self, m: int, n: int):
        &quot;&quot;&quot;
        Y = F(X) + B = W @ X + B,
        Y, B: R^n
        F:    R^m -&gt; R^n
        W:    R^mn
        X:    R^m
        W and B are randomly initialized
        &quot;&quot;&quot;
        self.W = random.randn(m, n)
        self.B = random.randn(n)

    def __call__(self, X: ndarray):
        &quot;Using this object as a function&quot;
        return self.W @ X + self.B
</code></pre>
<h2 id="2-a-hrefgetting-startedobjhtmlobjectivesa"><a class="header" href="#2-a-hrefgetting-startedobjhtmlobjectivesa">2. <a href="getting-started/./obj.html">Objectives</a></a></h2>
<p>Suppose that we have some real data to evaluate our model on. If there's no real data then the model doesn't matter right? Suppose we have \(R_X \in R^m\), and \(R_Y \in R^n \) as our \(X, Y\) labels. We want to use <strong>mean-squared-error</strong> as our <strong>loss function</strong>. Then the <strong>loss</strong> of our function would be \(\sum_{i=1}^{n}({F_i(R_X)} - {R_Y}_i)^2\).</p>
<p>Code:</p>
<pre><code class="language-python">    # Continued from the previous definition

    @staticmethod
    def loss_func(x, y):
        &quot;Calculate the loss between `x` and `y`&quot;
        squared = (x - y) ** 2
        return squared.sum()

    def loss(self, real_x, real_y):
        &quot;Calculate the loss given a pair of real data&quot;

        # Calling the above defined `__call__` method
        pred_y = self(real_x)

        # See how far the predict value is from the real value
        # That is how we evaluate how good your model is
        return self.loss_func(pred_y, real_y)
</code></pre>
<h2 id="3-a-hrefgetting-startedoptimhtmloptimizationa-1"><a class="header" href="#3-a-hrefgetting-startedoptimhtmloptimizationa-1">3. <a href="getting-started/./optim.html">Optimization</a></a></h2>
<p>We want to find the gradient of the loss (\(y\) in the last chapter) with respect to the model parameters (\(x\) in the last chapter, <code>self.W</code>, <code>self.B</code> in the code). With a little aid of chain rule, we will find out the gradient of model output \(Y\) with respect to the model parameters \(W, B\).</p>
<p>In partial derivative form:</p>
<p>\[\frac{\partial Y_i}{\partial W_{ij}} = X_j\]
\[\frac{\partial Y_i}{\partial B_i} = 1\]</p>
<p>Since the loss \(L\) is defined as:</p>
<p>\[l = \sum_{i=1}^n (Y_i - {R_Y}_i)^2\]</p>
<p>The gradient of \(L\) with respect to the model parameters \(W, B\) is (in partial derivative form):</p>
<p>\[\frac{\partial L}{\partial W_{ij}} = \frac{\partial L}{\partial Y_i} \frac{\partial Y_i}{\partial W_{ij}} = (Y_i - {R_Y}_i)(X_j)\]</p>
<p>\[\frac{\partial L}{\partial B_i} = \frac{\partial L}{\partial Y_i} \frac{\partial Y_i}{\partial B_i} = (Y_i - {R_Y}_i)\]</p>
<p>Voila! Finally we get the answer. In tool-kits like PyTorch or TensorFlow though, the action is performed automatically by building a graph. So we don't suffer from this pain every time we want to write a program!</p>
<p>Code:</p>
<pre><code class="language-python">    # Continued from the previous definition

    def gradient(self, real_x, real_y):
        &quot;Calculate the gradient&quot;

        # Calling the above defined `__call__` method
        pred_y = self(real_x)

        # Getting the element-wise difference
        diff = pred_y - real_y

        # partial L / partial W_ij = diff_i * x_j
        W_grad = real_x.T @ diff

        # partial L / partial B_i = diff_i
        B_grad = diff

        return Linear.Grad(w=W_grad, b=B_grad)

    def train(self, real_x, real_y, lr):
        &quot;This is what people call training a model&quot;
        grad = self.gradient(real_x, real_y)

        # updating the parameters
        self.W -= lr * grad.w
        self.B -= lr * grad.b
</code></pre>
<h2 id="summary-4"><a class="header" href="#summary-4">Summary</a></h2>
<p>We have seen how the three essential parts in a training procedure work. Hopefully by now you have a better understanding of how things work in machine learning fashion.</p>
<h1 id="commonly-confused-concepts"><a class="header" href="#commonly-confused-concepts">Commonly confused concepts</a></h1>
<h1 id="correctness"><a class="header" href="#correctness">Correctness</a></h1>
<h1 id="simplicity"><a class="header" href="#simplicity">Simplicity</a></h1>
<h1 id="commonly-confused-concepts-1"><a class="header" href="#commonly-confused-concepts-1">Commonly confused concepts</a></h1>
<h1 id="how-machine-learning-works"><a class="header" href="#how-machine-learning-works">How machine learning works</a></h1>
<p>By now you understand what the three main components of machine learning are. </p>
<h1 id="machine-learning-is-not-that-special"><a class="header" href="#machine-learning-is-not-that-special">Machine learning is not that special</a></h1>
<p><em>What I cannot create, I cannot understand.</em></p>
<h1 id="what-functions-can-be-approximated"><a class="header" href="#what-functions-can-be-approximated">What functions can be approximated?</a></h1>
<h1 id="commonly-confused-concepts-2"><a class="header" href="#commonly-confused-concepts-2">Commonly confused concepts</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        
        <script type="text/javascript">
            window.playground_line_numbers = true;
        </script>
        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        
        <script src="ace.js" type="text/javascript" charset="utf-8"></script>
        <script src="editor.js" type="text/javascript" charset="utf-8"></script>
        <script src="mode-rust.js" type="text/javascript" charset="utf-8"></script>
        <script src="theme-dawn.js" type="text/javascript" charset="utf-8"></script>
        <script src="theme-tomorrow_night.js" type="text/javascript" charset="utf-8"></script>
        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
        
        

    </body>
</html>
