
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Transformer Block &#8212; Learning Machine</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="canonical" href="https://rentruewang.github.io/layers/transformer/transformer.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Attention" href="attn/attn.html" />
    <link rel="prev" title="Pooling Layer" href="../pooling/pooling.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning Machine</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Introduction
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Getting started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../basics/basics.html">
   Holy Trinity for Machine Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/data/data.html">
     Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/model/model.html">
     Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/loss/loss.html">
     Loss Function
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../basics/approx/approx.html">
   Approximation models
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../basics/gradients/gradients.html">
   Gradients
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/gradients/loss-fn-derivative.html">
     Do loss functions have to be differentiable?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/gradients/torch-cal-grads.html">
     How Gradients Are Calculated?
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Common Tasks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../tasks/tasks.html">
   Types of tasks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tasks/regression/regression.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tasks/classification/classification.html">
   Classification
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Common Building Blocks
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../layers.html">
   Layers
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../linear/linear.html">
   Linear Layer
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../linear/linear-grad.html">
     Calculate gradients for Linear Layers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../cnn/cnn.html">
   Convolution Layer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rnn/rnn.html">
   Recurrent Layer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../emb/emb.html">
   Embedding Layer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dropout/dropout.html">
   Dropout Layer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../norm/norm.html">
   Normalization Layer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../padding/padding.html">
   Padding Layer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../pooling/pooling.html">
   Pooling Layer
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="current reference internal" href="#">
   Transformer Block
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="attn/attn.html">
     Attention
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="attn/self-attn.html">
     Self Attention
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="transformer-vs-rnn.html">
     Transformer vs RNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="training/training.html">
     Training Transformers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="training/teacher/teacher.html">
     Teacher Forcing vs Scheduled Sampling vs Normal Mode
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="training/token/token.html">
     Token
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="training/no-training/no-training.html">
     Using Bert without training?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../activation/activation.html">
   Activation Functions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../activation/relu/relu.html">
     Rectified Linear Unit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../activation/sigmoid/sigmoid.html">
     Sigmoid
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../activation/softmax/softmax.html">
     Softmax
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../activation/tanh/tanh.html">
     Hyperbolic Tangent
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Frequently encountered issues
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../issues/issues.html">
   Frequently Encoutered Issues
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../issues/batch/batch.html">
   Batch size
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../issues/gradient/norm.html">
   Gradient Vanishing / Gradient Explosion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../issues/gradient/saddle.html">
   Saddle point
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../issues/lr/lr.html">
   Learning Rate
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../issues/data/overfit.html">
   Overfit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../issues/data/underfit.html">
   Underfit
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Generative Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../generative/generative.html">
   Generative Models
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../generative/ae/ae.html">
   AutoEncoder Model
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../generative/ae/ae-arch.html">
     Auto Encoder Architecture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../generative/ae/ae-semi.html">
     Improving Auto Encoders with Semi Supervised Training
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../generative/ae/vae/vae.html">
     Variational AutoEncoder Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../generative/gan/gan.html">
   Generative Adversarial Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../generative/gmm/gmm.html">
   Gaussian Mixture Model
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Improvements to a model
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../better/better.html">
   Improvements to a model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../better/explainable/explainable.html">
   Explainable AI
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../better/meta/meta.html">
   Meta Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../better/lll/lll.html">
   Life Long Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../better/compression/compression.html">
   Model Compression
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Using existing models to improve models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../reuse/reuse.html">
   Reusing Existing Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reuse/da/da.html">
   Domain Adaptation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reuse/transfer/transfer.html">
   Transfer Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reuse/distil/distil.html">
   Knowledge Distilation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Beyond supervised training
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../not-supervised/semi-supervised/semi-supervised.html">
   Semi Supervised Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../not-supervised/unsupervised.html">
   Unsupervised Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../not-supervised/decision-tree/decision-tree.html">
   Decision Tree
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../not-supervised/clustering/clustering.html">
   Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../not-supervised/self-supervised/self-supervised.html">
   Self Supervised Learning
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/layers/transformer/transformer.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/rentruewang/learning-machine"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/rentruewang/learning-machine/issues/new?title=Issue%20on%20page%20%2Flayers/transformer/transformer.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/rentruewang/learning-machine/edit/main/book/layers/transformer/transformer.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/rentruewang/learning-machine/main?urlpath=tree/book/layers/transformer/transformer.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attention-is-all-you-need">
   Attention is all you need
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-does-the-transformer-work">
   How does the transformer work?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#transformer-encoder">
   Transformer encoder
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#transformer-decoder">
   Transformer decoder
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#positional-encoding">
   Positional encoding
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="transformer-block">
<h1>Transformer Block<a class="headerlink" href="#transformer-block" title="Permalink to this headline">¶</a></h1>
<div class="section" id="attention-is-all-you-need">
<h2>Attention is all you need<a class="headerlink" href="#attention-is-all-you-need" title="Permalink to this headline">¶</a></h2>
<p>The transformer model comes out of the paper <em>attention is all you need</em>. The paper shows how powerful pure attention mechanisms can be. They introduced a new kind of attention mechanism called self-attention, which we’ll discuss later.</p>
<p>The significance about self-attention is that with only attention mechanism, the model achieves state-of-the-art performance on many datasets, in a field previously dominated by RNNs.</p>
</div>
<div class="section" id="how-does-the-transformer-work">
<h2>How does the transformer work?<a class="headerlink" href="#how-does-the-transformer-work" title="Permalink to this headline">¶</a></h2>
<p>The transformer architecture is based on a Seq2Seq model. Traditionally, a seq2seq model is basically an encoder and a decoder, like auto-encoders, but both encoder and decoder are RNNs. The encoder first process through the input, then feeds the encoder’s RNN state or output to the decoder to decode the full sentence. The idea is that the encoder should be able to encode the input into some kind of representation that contains the meaning of the sentence, and the decoder should be able to understand that representation.</p>
<p>In the case of transformers, because it’s not an RNN, so instead of RNN state, the attention produced by the encoder is used and sent to the decoder. Decoder uses that global information to produce the output.</p>
</div>
<div class="section" id="transformer-encoder">
<h2>Transformer encoder<a class="headerlink" href="#transformer-encoder" title="Permalink to this headline">¶</a></h2>
<p>The encoding component is a stack of smaller encoders. An encoder does the following thing</p>
<ol class="simple">
<li><p>Calculate self-attention score for the input <span class="math notranslate nohighlight">\( I \)</span>.</p></li>
<li><p>Weigh the input by self-attention scores <span class="math notranslate nohighlight">\( S(I) \)</span>.</p></li>
<li><p>Pass it through an add-and-normalize layer <span class="math notranslate nohighlight">\( O = I + S(I) \)</span>.</p></li>
<li><p>Feed the processed data through a linear layer <span class="math notranslate nohighlight">\( F = f(O) \)</span>.</p></li>
<li><p>Perform activation on the linear layer’s output <span class="math notranslate nohighlight">\( F' = \sigma(F) \)</span>.</p></li>
<li><p>Multiply the mutated output with the output itself <span class="math notranslate nohighlight">\( F'' = F'F \)</span>.</p></li>
<li><p>Pass it through an add-and-normalize layer <span class="math notranslate nohighlight">\( F'' + O \)</span>.</p></li>
</ol>
<p>An add-and-normalize layer performs a residual add, adding the input and the processed input.</p>
</div>
<div class="section" id="transformer-decoder">
<h2>Transformer decoder<a class="headerlink" href="#transformer-decoder" title="Permalink to this headline">¶</a></h2>
<p>The decoding component look a lot like the encoding component, it’s also a stack of decoders. Decoders are basically encoders, but they take the attention provided by encoders, and perform step <span class="math notranslate nohighlight">\( 3 \)</span> twice, the first time adds it by the decoder generated attention, the second time adds by the encoder generated attention.</p>
<p>For transformers, the decoder is an auto-regressive model. In inference mode, what it does is no different from any other decoder. It takes in the sequence it previously generated (it uses <bos> it’s the first token being predicted), and predicts a new token. So what is the encoder doing? Turns out it is used for storing attention information. The stored information is then passed to decoders (on different layers) so that decoders know the meaning of the input sentence.</p>
</div>
<div class="section" id="positional-encoding">
<h2>Positional encoding<a class="headerlink" href="#positional-encoding" title="Permalink to this headline">¶</a></h2>
<p>Words in a sentence have different meanings if they are ordered differently. The sentence <em>Alice ate Bob</em> is very different in meaning to <em>Bob ate Alice</em>. Well, at least for Alice and Bob. For RNNs, that isn’t an issue. Because RNNs run over a sentence sequentially. So it will either see Alice or Bob first, and knows who appears to be eaten. However, transformers have no way of knowing who comes first because of how self-attention mechanism is symmetry to each position.</p>
<p>That is the reason we need to add information for position to the model. In <em>Attention is all you need</em>, a positional encoding is added to the input. A positional encoding is basically an embedding, with different values for every indices, so that the model knows what a word’s position is when it processes it.</p>
<p>A very interesting fact is that changing the order of the tokens does not actually change the output of the model (unlike RNNs), as long as the right positional encoding is associated with the right position. That means that after applying (usually by adding) positional encoding to the input word embedding vector, you can shuffle the order of the vector (along the time axis) all you want without affecting the output of the model. Very cool indeed.</p>
</div>
<div class="toctree-wrapper compound">
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./layers/transformer"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../pooling/pooling.html" title="previous page">Pooling Layer</a>
    <a class='right-next' id="next-link" href="attn/attn.html" title="next page">Attention</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By RenChu Wang<br/>
        
            &copy; Copyright RenChu Wang, 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>